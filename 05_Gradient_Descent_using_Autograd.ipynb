{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "05-Gradient_Descent_using_Autograd.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPdtq2pULvJ6SiI5okh+PF9"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "show an example of how we optimize our model with automatic gradient computaton.\n",
        "\n",
        "steps:\n",
        "\n",
        "(first we do all steps manually)\n",
        "\n",
        "then:\n",
        "\n",
        "- prediction: Pytorch Model\n",
        "\n",
        "- gradients computation: Autograd\n",
        "\n",
        "- Loss computation: pytorch Loss\n",
        "\n",
        "- parameter updates: pytorch optimizer"
      ],
      "metadata": {
        "id": "dRtWD2-j5D3w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **From scratch**"
      ],
      "metadata": {
        "id": "_tBVvAgJ6d0S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "Z59uUO2f5KjZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# f = w * x\n",
        "# f = 2 * x\n",
        "\n",
        "X = np.array([1,2,3,4], dtype=np.float32)\n",
        "Y = np.array([2,4,6,8], dtype=np.float32)\n",
        "\n",
        "w = 0.0\n",
        "\n",
        "# Calculate our model prediction\n",
        "def forward(x):\n",
        "  return w * x\n",
        "\n",
        "\n",
        "# Loss = MSE\n",
        "def loss(y,y_predicted):\n",
        "  return ((y_predicted - y)**2).mean()\n",
        "\n",
        "\n",
        "# gradients\n",
        "# MSE = 1/N * (w*x - y)**2\n",
        "#dJ/dw = 1/N * 2x * (w*x - y)\n",
        "def gradient(x,y,y_predicted):\n",
        "  return np.dot(2*x , y_predicted - y).mean()"
      ],
      "metadata": {
        "id": "fhHW_HJT6lik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Prediction before trainig: f(5) = {forward(5):.3f}\\n\")\n",
        "\n",
        "#training\n",
        "learning_rate = 0.01\n",
        "n_iter = 20\n",
        "\n",
        "for epoch in range(n_iter):\n",
        "  #prediction (forward pass)\n",
        "  y_pred = forward(X)\n",
        "\n",
        "  #loss\n",
        "  l = loss(Y,y_pred)\n",
        "\n",
        "  #gradients\n",
        "  dw = gradient(X, Y, y_pred)\n",
        "\n",
        "  #update weights\n",
        "  w -= learning_rate * dw\n",
        "\n",
        "  if epoch%2 ==0 :\n",
        "    print(f'epoch {epoch+1}: w = {w:.3f}, loss = {l:.8f}')\n",
        "\n",
        "\n",
        "\n",
        "print(f\"Prediction after trainig: f(5) = {forward(5):.3f}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "muzhrCAFA-U1",
        "outputId": "e152e32c-a226-455d-8691-44b3c1d5bc38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction before trainig: f(5) = 0.000\n",
            "\n",
            "epoch 1: w = 1.200, loss = 30.00000000\n",
            "epoch 3: w = 1.872, loss = 0.76800019\n",
            "epoch 5: w = 1.980, loss = 0.01966083\n",
            "epoch 7: w = 1.997, loss = 0.00050331\n",
            "epoch 9: w = 1.999, loss = 0.00001288\n",
            "epoch 11: w = 2.000, loss = 0.00000033\n",
            "epoch 13: w = 2.000, loss = 0.00000001\n",
            "epoch 15: w = 2.000, loss = 0.00000000\n",
            "epoch 17: w = 2.000, loss = 0.00000000\n",
            "epoch 19: w = 2.000, loss = 0.00000000\n",
            "Prediction after trainig: f(5) = 10.000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **with pytorch**"
      ],
      "metadata": {
        "id": "WEbUO6VAFQlf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "kyojnpTRFTEn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# f = w * x\n",
        "# f = 2 * x\n",
        "\n",
        "X = torch.tensor([1,2,3,4], dtype=torch.float32)\n",
        "Y = torch.tensor([2,4,6,8], dtype=torch.float32)\n",
        "\n",
        "w = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)\n",
        "\n",
        "# Calculate our model prediction\n",
        "def forward(x):\n",
        "  return w * x\n",
        "\n",
        "\n",
        "# Loss = MSE\n",
        "def loss(y,y_predicted):\n",
        "  return ((y_predicted - y)**2).mean()\n",
        "\n",
        "\n",
        "# gradients"
      ],
      "metadata": {
        "id": "vpbL4WfDFVAE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Prediction before trainig: f(5) = {forward(5):.3f}\\n\")\n",
        "\n",
        "#training\n",
        "learning_rate = 0.01\n",
        "n_iter = 1000\n",
        "\n",
        "for epoch in range(n_iter):\n",
        "  #prediction (forward pass)\n",
        "  y_pred = forward(X)\n",
        "\n",
        "  #loss\n",
        "  l = loss(Y,y_pred)\n",
        "\n",
        "  #gradients (backward pass)\n",
        "  l.backward() #calculate dLoss/dw\n",
        "\n",
        "  #update weights\n",
        "  with torch.no_grad():\n",
        "    w -= learning_rate * w.grad #because this isn't in torch graph\n",
        "\n",
        "    # zero gradients\n",
        "    w.grad.zero_()\n",
        "\n",
        "  if epoch%10 ==0 :\n",
        "    print(f'epoch {epoch+1}: w = {w:.3f}, loss = {l:.8f}')\n",
        "\n",
        "\n",
        "print(f\"Prediction after trainig: f(5) = {forward(5):.3f}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7rLIjqaCGS2i",
        "outputId": "3d24af3e-a6f0-4742-a877-410732b35b8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction before trainig: f(5) = 0.000\n",
            "\n",
            "epoch 1: w = 0.300, loss = 30.00000000\n",
            "epoch 11: w = 1.665, loss = 1.16278565\n",
            "epoch 21: w = 1.934, loss = 0.04506890\n",
            "epoch 31: w = 1.987, loss = 0.00174685\n",
            "epoch 41: w = 1.997, loss = 0.00006770\n",
            "epoch 51: w = 1.999, loss = 0.00000262\n",
            "epoch 61: w = 2.000, loss = 0.00000010\n",
            "epoch 71: w = 2.000, loss = 0.00000000\n",
            "epoch 81: w = 2.000, loss = 0.00000000\n",
            "epoch 91: w = 2.000, loss = 0.00000000\n",
            "epoch 101: w = 2.000, loss = 0.00000000\n",
            "epoch 111: w = 2.000, loss = 0.00000000\n",
            "epoch 121: w = 2.000, loss = 0.00000000\n",
            "epoch 131: w = 2.000, loss = 0.00000000\n",
            "epoch 141: w = 2.000, loss = 0.00000000\n",
            "epoch 151: w = 2.000, loss = 0.00000000\n",
            "epoch 161: w = 2.000, loss = 0.00000000\n",
            "epoch 171: w = 2.000, loss = 0.00000000\n",
            "epoch 181: w = 2.000, loss = 0.00000000\n",
            "epoch 191: w = 2.000, loss = 0.00000000\n",
            "epoch 201: w = 2.000, loss = 0.00000000\n",
            "epoch 211: w = 2.000, loss = 0.00000000\n",
            "epoch 221: w = 2.000, loss = 0.00000000\n",
            "epoch 231: w = 2.000, loss = 0.00000000\n",
            "epoch 241: w = 2.000, loss = 0.00000000\n",
            "epoch 251: w = 2.000, loss = 0.00000000\n",
            "epoch 261: w = 2.000, loss = 0.00000000\n",
            "epoch 271: w = 2.000, loss = 0.00000000\n",
            "epoch 281: w = 2.000, loss = 0.00000000\n",
            "epoch 291: w = 2.000, loss = 0.00000000\n",
            "epoch 301: w = 2.000, loss = 0.00000000\n",
            "epoch 311: w = 2.000, loss = 0.00000000\n",
            "epoch 321: w = 2.000, loss = 0.00000000\n",
            "epoch 331: w = 2.000, loss = 0.00000000\n",
            "epoch 341: w = 2.000, loss = 0.00000000\n",
            "epoch 351: w = 2.000, loss = 0.00000000\n",
            "epoch 361: w = 2.000, loss = 0.00000000\n",
            "epoch 371: w = 2.000, loss = 0.00000000\n",
            "epoch 381: w = 2.000, loss = 0.00000000\n",
            "epoch 391: w = 2.000, loss = 0.00000000\n",
            "epoch 401: w = 2.000, loss = 0.00000000\n",
            "epoch 411: w = 2.000, loss = 0.00000000\n",
            "epoch 421: w = 2.000, loss = 0.00000000\n",
            "epoch 431: w = 2.000, loss = 0.00000000\n",
            "epoch 441: w = 2.000, loss = 0.00000000\n",
            "epoch 451: w = 2.000, loss = 0.00000000\n",
            "epoch 461: w = 2.000, loss = 0.00000000\n",
            "epoch 471: w = 2.000, loss = 0.00000000\n",
            "epoch 481: w = 2.000, loss = 0.00000000\n",
            "epoch 491: w = 2.000, loss = 0.00000000\n",
            "epoch 501: w = 2.000, loss = 0.00000000\n",
            "epoch 511: w = 2.000, loss = 0.00000000\n",
            "epoch 521: w = 2.000, loss = 0.00000000\n",
            "epoch 531: w = 2.000, loss = 0.00000000\n",
            "epoch 541: w = 2.000, loss = 0.00000000\n",
            "epoch 551: w = 2.000, loss = 0.00000000\n",
            "epoch 561: w = 2.000, loss = 0.00000000\n",
            "epoch 571: w = 2.000, loss = 0.00000000\n",
            "epoch 581: w = 2.000, loss = 0.00000000\n",
            "epoch 591: w = 2.000, loss = 0.00000000\n",
            "epoch 601: w = 2.000, loss = 0.00000000\n",
            "epoch 611: w = 2.000, loss = 0.00000000\n",
            "epoch 621: w = 2.000, loss = 0.00000000\n",
            "epoch 631: w = 2.000, loss = 0.00000000\n",
            "epoch 641: w = 2.000, loss = 0.00000000\n",
            "epoch 651: w = 2.000, loss = 0.00000000\n",
            "epoch 661: w = 2.000, loss = 0.00000000\n",
            "epoch 671: w = 2.000, loss = 0.00000000\n",
            "epoch 681: w = 2.000, loss = 0.00000000\n",
            "epoch 691: w = 2.000, loss = 0.00000000\n",
            "epoch 701: w = 2.000, loss = 0.00000000\n",
            "epoch 711: w = 2.000, loss = 0.00000000\n",
            "epoch 721: w = 2.000, loss = 0.00000000\n",
            "epoch 731: w = 2.000, loss = 0.00000000\n",
            "epoch 741: w = 2.000, loss = 0.00000000\n",
            "epoch 751: w = 2.000, loss = 0.00000000\n",
            "epoch 761: w = 2.000, loss = 0.00000000\n",
            "epoch 771: w = 2.000, loss = 0.00000000\n",
            "epoch 781: w = 2.000, loss = 0.00000000\n",
            "epoch 791: w = 2.000, loss = 0.00000000\n",
            "epoch 801: w = 2.000, loss = 0.00000000\n",
            "epoch 811: w = 2.000, loss = 0.00000000\n",
            "epoch 821: w = 2.000, loss = 0.00000000\n",
            "epoch 831: w = 2.000, loss = 0.00000000\n",
            "epoch 841: w = 2.000, loss = 0.00000000\n",
            "epoch 851: w = 2.000, loss = 0.00000000\n",
            "epoch 861: w = 2.000, loss = 0.00000000\n",
            "epoch 871: w = 2.000, loss = 0.00000000\n",
            "epoch 881: w = 2.000, loss = 0.00000000\n",
            "epoch 891: w = 2.000, loss = 0.00000000\n",
            "epoch 901: w = 2.000, loss = 0.00000000\n",
            "epoch 911: w = 2.000, loss = 0.00000000\n",
            "epoch 921: w = 2.000, loss = 0.00000000\n",
            "epoch 931: w = 2.000, loss = 0.00000000\n",
            "epoch 941: w = 2.000, loss = 0.00000000\n",
            "epoch 951: w = 2.000, loss = 0.00000000\n",
            "epoch 961: w = 2.000, loss = 0.00000000\n",
            "epoch 971: w = 2.000, loss = 0.00000000\n",
            "epoch 981: w = 2.000, loss = 0.00000000\n",
            "epoch 991: w = 2.000, loss = 0.00000000\n",
            "Prediction after trainig: f(5) = 10.000\n",
            "\n"
          ]
        }
      ]
    }
  ]
}